

# llama


```
git clone https://github.com/meta-llama/llama.git
```
```
./download.sh
```


## model downloader:

+ [model_downloader](./llm-run/test.py)
```py
from llm_run import md_downloader


md_downloader.install("mlx-community/Llama-2-7b-chat-4-bit")
```


## llama.cpp :

### llama.cpp-python :

```
pip install llama-cpp-python
```

## referance :

+[llama-2 readme](https://github.com/meta-llama/llama/tree/main?tab=readme-ov-file#llama-2)
+[llama.cpp-python](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#-python-bindings-for-llamacpp)
+[reddit-post/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/11o6o3f/how_to_install_llama_8bit_and_4bit/)
+[text-generation-webui](https://github.com/oobabooga/text-generation-webui/tree/main?tab=readme-ov-file#text-generation-web-ui)
